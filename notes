TODOS:
config.py line 10
app.py lines 21, 80, 86



        to run the server:
            ----------
uvicorn app:app --host 0.0.0.0 --port 8000
            ----------


        liveness check:
            ----------
curl http://127.0.0.1:8000/health
            ----------


        /chat test:
            ----------
curl -s -X POST http://127.0.0.1:8000/chat -H "Content-Type: application/json" -d '{"message":"say hello","agent":"general","stream":false}'
            ----------




next steps:



# Streaming + short memory-
1) *Streaming responses**

* Implement `stream=True` path (Server-Sent Events or chunked text). In Ollama, stream endpoint returns multiple JSON lines; yield as they arrive.
* Acceptance: `curl` or a tiny web client sees tokens arrive incrementally; non-stream unchanged.

 *Conversation memory (ephemeral)**

* `conversation_id` in request; store last N turns in a dict with FIFO trim respecting `CTX_TOKENS`.
* Compose prompt as: system + compressed history + new user.
* Acceptance: asking follow-ups within same `conversation_id` uses prior turns; memory resets if new id.





2) **Structured logging**

* Add request-ID per call; log: time, agent, provider, model, latency\_ms, tokens\_in/out (approx), status.
* Acceptance: each `/chat` call prints one JSON log line.

**Provider options mapping**

* Map config caps to Ollama options (`num_ctx=CTX_TOKENS`, `num_predict=MAX_TOKENS`, `temperature=TEMPERATURE`).
* Acceptance: options appear in the payload; server still answers correctly.

**Graceful error surface**

* Global exception handlers: `ProviderError`→502, `RequestValidationError`→422 (already handled by FastAPI), unknown→500 (redacted message).
* Acceptance: bad model name shows 502 with safe text; logs have full traceback.







changes made this version:
1) phase 2 section in utils/config.py
2) added utils/memory.py
3) changed agents/base.py (actually include recent history in prompts)
4) changed providers/base.py (allow streaming return types)
5) providers/ollama.py implement streaming path
6) app.py add streaming path + memory
7) .env modified