
next steps:




2) **Structured logging**

* Add request-ID per call; log: time, agent, provider, model, latency\_ms, tokens\_in/out (approx), status.
* Acceptance: each `/chat` call prints one JSON log line.

**Provider options mapping**

* Map config caps to Ollama options (`num_ctx=CTX_TOKENS`, `num_predict=MAX_TOKENS`, `temperature=TEMPERATURE`).
* Acceptance: options appear in the payload; server still answers correctly.

**Graceful error surface**

* Global exception handlers: `ProviderError`→502, `RequestValidationError`→422 (already handled by FastAPI), unknown→500 (redacted message).
* Acceptance: bad model name shows 502 with safe text; logs have full traceback.







changes made this version:
1) phase 2 section in utils/config.py
2) added utils/memory.py
3) changed agents/base.py (actually include recent history in prompts)
4) changed providers/base.py (allow streaming return types)
5) providers/ollama.py implement streaming path
6) app.py add streaming path + memory
7) .env modified